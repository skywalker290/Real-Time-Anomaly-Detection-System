{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UCF101_subset.tar.gz'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import os\n",
    "hf_dataset_identifier = \"sayakpaul/ucf101-subset\"\n",
    "filename = \"UCF101_subset.tar.gz\"\n",
    "file_path = hf_hub_download(repo_id=hf_dataset_identifier, filename=filename, repo_type=\"dataset\", local_dir=\".\")\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "with tarfile.open(\"UCF101_subset.tar.gz\") as t:\n",
    "     t.extractall(\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-11-09 13:51:32--  https://github.com/olonok69/LLM_Notebooks/raw/main/video/fine_tune_ViViT/data_handling.py\n",
      "Resolving github.com (github.com)... 20.207.73.82\n",
      "Connecting to github.com (github.com)|20.207.73.82|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/olonok69/LLM_Notebooks/main/video/fine_tune_ViViT/data_handling.py [following]\n",
      "--2024-11-09 13:51:32--  https://raw.githubusercontent.com/olonok69/LLM_Notebooks/main/video/fine_tune_ViViT/data_handling.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8003::154, 2606:50c0:8001::154, 2606:50c0:8002::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8003::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2915 (2.8K) [text/plain]\n",
      "Saving to: ‘data_handling.py.1’\n",
      "\n",
      "data_handling.py.1  100%[===================>]   2.85K  --.-KB/s    in 0s      \n",
      "\n",
      "2024-11-09 13:51:33 (22.7 MB/s) - ‘data_handling.py.1’ saved [2915/2915]\n",
      "\n",
      "--2024-11-09 13:51:33--  https://github.com/olonok69/LLM_Notebooks/raw/main/video/fine_tune_ViViT/model_configuration.py\n",
      "Resolving github.com (github.com)... 20.207.73.82\n",
      "Connecting to github.com (github.com)|20.207.73.82|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/olonok69/LLM_Notebooks/main/video/fine_tune_ViViT/model_configuration.py [following]\n",
      "--2024-11-09 13:51:33--  https://raw.githubusercontent.com/olonok69/LLM_Notebooks/main/video/fine_tune_ViViT/model_configuration.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8003::154, 2606:50c0:8000::154, 2606:50c0:8002::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8003::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1139 (1.1K) [text/plain]\n",
      "Saving to: ‘model_configuration.py.1’\n",
      "\n",
      "model_configuration 100%[===================>]   1.11K  --.-KB/s    in 0s      \n",
      "\n",
      "2024-11-09 13:51:33 (38.7 MB/s) - ‘model_configuration.py.1’ saved [1139/1139]\n",
      "\n",
      "--2024-11-09 13:51:33--  https://github.com/olonok69/LLM_Notebooks/raw/main/video/fine_tune_ViViT/preprocessing.py\n",
      "Resolving github.com (github.com)... 20.207.73.82\n",
      "Connecting to github.com (github.com)|20.207.73.82|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/olonok69/LLM_Notebooks/main/video/fine_tune_ViViT/preprocessing.py [following]\n",
      "--2024-11-09 13:51:34--  https://raw.githubusercontent.com/olonok69/LLM_Notebooks/main/video/fine_tune_ViViT/preprocessing.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8002::154, 2606:50c0:8000::154, 2606:50c0:8003::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8002::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1018 [text/plain]\n",
      "Saving to: ‘preprocessing.py.1’\n",
      "\n",
      "preprocessing.py.1  100%[===================>]    1018  --.-KB/s    in 0s      \n",
      "\n",
      "2024-11-09 13:51:34 (18.8 MB/s) - ‘preprocessing.py.1’ saved [1018/1018]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/olonok69/LLM_Notebooks/raw/main/video/fine_tune_ViViT/data_handling.py\n",
    "!wget https://github.com/olonok69/LLM_Notebooks/raw/main/video/fine_tune_ViViT/model_configuration.py\n",
    "!wget https://github.com/olonok69/LLM_Notebooks/raw/main/video/fine_tune_ViViT/preprocessing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers datasets==2.21.0 huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/darth/.pyenv/versions/3.10.12/envs/major/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer, TrainingArguments, AdamW\n",
    "from model_configuration import *\n",
    "from transformers import Trainer\n",
    "from preprocessing import create_dataset\n",
    "from data_handling import frames_convert_and_create_dataset_dictionary\n",
    "from model_configuration import initialise_model\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "env_path =  \".env\"\n",
    "load_dotenv(env_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_configuration\n",
    "from model_configuration import compute_metrics\n",
    "import cv2\n",
    "import av\n",
    "from data_handling import sample_frame_indices, read_video_pyav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "container = av.open(\"./data/UCF101_subset/test/ApplyEyeMakeup/v_ApplyEyeMakeup_g03_c01.avi\")\n",
    "container.streams.video[0].frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install moviepy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import moviepy.editor\n",
    "container = av.open(\"./data/UCF101_subset/test/ApplyEyeMakeup/v_ApplyEyeMakeup_g03_c01.avi\")\n",
    "indices = sample_frame_indices(clip_len=50, frame_sample_rate=2,seg_len=container.streams.video[0].frames)\n",
    "video = read_video_pyav(container=container, indices=indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 74,  76,  78,  80,  82,  84,  86,  88,  90,  92,  94,  96,  98,\n",
       "       100, 102, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 125,\n",
       "       127, 129, 131, 133, 135, 137, 139, 141, 143, 145, 147, 149, 151,\n",
       "       153, 155, 157, 159, 161, 163, 165, 167, 169, 171, 173])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 224, 224, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g08_c04.avi number of Frames: 95\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g20_c05.avi number of Frames: 178\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g23_c01.avi number of Frames: 116\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g20_c07.avi number of Frames: 250\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g17_c01.avi number of Frames: 141\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g06_c04.avi number of Frames: 113\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g02_c03.avi number of Frames: 117\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g25_c01.avi number of Frames: 293\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g20_c03.avi number of Frames: 263\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g15_c05.avi number of Frames: 217\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g12_c03.avi number of Frames: 96\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g05_c01.avi number of Frames: 221\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g11_c01.avi number of Frames: 106\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g14_c03.avi number of Frames: 176\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g07_c06.avi number of Frames: 210\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g24_c01.avi number of Frames: 194\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g15_c01.avi number of Frames: 198\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g24_c05.avi number of Frames: 112\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g07_c02.avi number of Frames: 145\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g16_c05.avi number of Frames: 223\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g11_c03.avi number of Frames: 99\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g02_c05.avi number of Frames: 84\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g18_c02.avi number of Frames: 227\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g01_c03.avi number of Frames: 156\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g10_c04.avi number of Frames: 184\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g14_c01.avi number of Frames: 122\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g02_c01.avi number of Frames: 109\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g08_c02.avi number of Frames: 111\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g17_c05.avi number of Frames: 109\n",
      "Processing file data/UCF101_subset/train/BabyCrawling/v_BabyCrawling_g15_c03.avi number of Frames: 101\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c06.avi number of Frames: 188\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g19_c02.avi number of Frames: 243\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g15_c06.avi number of Frames: 197\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g25_c07.avi number of Frames: 145\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g24_c02.avi number of Frames: 105\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g06_c03.avi number of Frames: 245\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g22_c05.avi number of Frames: 243\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g25_c05.avi number of Frames: 93\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g04_c07.avi number of Frames: 158\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g18_c03.avi number of Frames: 157\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g02_c03.avi number of Frames: 216\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g07_c06.avi number of Frames: 162\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g16_c03.avi number of Frames: 114\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g10_c01.avi number of Frames: 153\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g25_c03.avi number of Frames: 125\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g12_c05.avi number of Frames: 127\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g21_c04.avi number of Frames: 227\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g11_c04.avi number of Frames: 138\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g18_c01.avi number of Frames: 159\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g07_c04.avi number of Frames: 142\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g13_c05.avi number of Frames: 119\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g11_c02.avi number of Frames: 178\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g22_c01.avi number of Frames: 170\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g05_c06.avi number of Frames: 115\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c02.avi number of Frames: 258\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g17_c04.avi number of Frames: 105\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g10_c05.avi number of Frames: 120\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g05_c02.avi number of Frames: 264\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g25_c01.avi number of Frames: 241\n",
      "Processing file data/UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c01.avi number of Frames: 120\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g23_c05.avi number of Frames: 91\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g06_c05.avi number of Frames: 130\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g15_c02.avi number of Frames: 184\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g25_c04.avi number of Frames: 259\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g07_c01.avi number of Frames: 91\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g14_c04.avi number of Frames: 244\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g02_c01.avi number of Frames: 158\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g13_c03.avi number of Frames: 160\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g07_c05.avi number of Frames: 64\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g02_c05.avi number of Frames: 127\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g20_c05.avi number of Frames: 120\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g06_c03.avi number of Frames: 87\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g05_c04.avi number of Frames: 51\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g24_c06.avi number of Frames: 122\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g04_c03.avi number of Frames: 140\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g11_c06.avi number of Frames: 243\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g19_c03.avi number of Frames: 159\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g11_c04.avi number of Frames: 258\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g13_c07.avi number of Frames: 190\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g10_c03.avi number of Frames: 203\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g06_c01.avi number of Frames: 81\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g03_c02.avi number of Frames: 176\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g08_c01.avi number of Frames: 363\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g09_c02.avi number of Frames: 83\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g04_c01.avi number of Frames: 165\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g01_c04.avi number of Frames: 125\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g22_c04.avi number of Frames: 73\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g24_c04.avi number of Frames: 121\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g10_c05.avi number of Frames: 188\n",
      "Processing file data/UCF101_subset/train/Archery/v_Archery_g22_c02.avi number of Frames: 70\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g18_c04.avi number of Frames: 119\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g02_c03.avi number of Frames: 98\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g23_c03.avi number of Frames: 135\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g21_c03.avi number of Frames: 143\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g22_c06.avi number of Frames: 172\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g07_c02.avi number of Frames: 125\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g15_c03.avi number of Frames: 133\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g21_c01.avi number of Frames: 85\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g04_c05.avi number of Frames: 169\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g05_c02.avi number of Frames: 154\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g22_c02.avi number of Frames: 176\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g02_c01.avi number of Frames: 127\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g09_c04.avi number of Frames: 210\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g22_c04.avi number of Frames: 177\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g12_c04.avi number of Frames: 277\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g17_c03.avi number of Frames: 142\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g19_c04.avi number of Frames: 155\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g01_c04.avi number of Frames: 369\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g11_c04.avi number of Frames: 157\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g04_c01.avi number of Frames: 98\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g03_c03.avi number of Frames: 154\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g01_c02.avi number of Frames: 160\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g03_c01.avi number of Frames: 180\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g18_c02.avi number of Frames: 139\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g24_c05.avi number of Frames: 288\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g13_c03.avi number of Frames: 164\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g19_c02.avi number of Frames: 111\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g06_c03.avi number of Frames: 153\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g08_c02.avi number of Frames: 219\n",
      "Processing file data/UCF101_subset/train/ApplyLipstick/v_ApplyLipstick_g17_c01.avi number of Frames: 161\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g03_c01.avi number of Frames: 197\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g18_c03.avi number of Frames: 181\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g01_c03.avi number of Frames: 112\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g21_c05.avi number of Frames: 115\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g10_c02.avi number of Frames: 69\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g15_c03.avi number of Frames: 125\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g24_c03.avi number of Frames: 100\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g06_c05.avi number of Frames: 102\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g03_c03.avi number of Frames: 101\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g21_c03.avi number of Frames: 125\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g19_c01.avi number of Frames: 153\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g05_c01.avi number of Frames: 95\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g17_c01.avi number of Frames: 191\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g23_c04.avi number of Frames: 107\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g06_c01.avi number of Frames: 106\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g25_c01.avi number of Frames: 80\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g08_c04.avi number of Frames: 116\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g06_c03.avi number of Frames: 105\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g09_c02.avi number of Frames: 79\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g07_c02.avi number of Frames: 110\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g05_c03.avi number of Frames: 111\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g02_c03.avi number of Frames: 60\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g04_c01.avi number of Frames: 120\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g12_c04.avi number of Frames: 96\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g04_c03.avi number of Frames: 160\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g14_c03.avi number of Frames: 201\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g14_c01.avi number of Frames: 145\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g08_c02.avi number of Frames: 125\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g10_c04.avi number of Frames: 122\n",
      "Processing file data/UCF101_subset/train/BalanceBeam/v_BalanceBeam_g19_c03.avi number of Frames: 118\n",
      "Processing file data/UCF101_subset/val/BabyCrawling/v_BabyCrawling_g09_c06.avi number of Frames: 119\n",
      "Processing file data/UCF101_subset/val/BabyCrawling/v_BabyCrawling_g21_c04.avi number of Frames: 189\n",
      "Processing file data/UCF101_subset/val/BabyCrawling/v_BabyCrawling_g13_c05.avi number of Frames: 85\n",
      "Processing file data/UCF101_subset/val/ApplyEyeMakeup/v_ApplyEyeMakeup_g14_c05.avi number of Frames: 160\n",
      "Processing file data/UCF101_subset/val/ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01.avi number of Frames: 164\n",
      "Processing file data/UCF101_subset/val/ApplyEyeMakeup/v_ApplyEyeMakeup_g20_c04.avi number of Frames: 220\n",
      "Processing file data/UCF101_subset/val/Archery/v_Archery_g18_c02.avi number of Frames: 291\n",
      "Processing file data/UCF101_subset/val/Archery/v_Archery_g18_c06.avi number of Frames: 160\n",
      "Processing file data/UCF101_subset/val/Archery/v_Archery_g12_c03.avi number of Frames: 436\n",
      "Processing file data/UCF101_subset/val/ApplyLipstick/v_ApplyLipstick_g10_c04.avi number of Frames: 247\n",
      "Processing file data/UCF101_subset/val/ApplyLipstick/v_ApplyLipstick_g20_c04.avi number of Frames: 140\n",
      "Processing file data/UCF101_subset/val/ApplyLipstick/v_ApplyLipstick_g25_c02.avi number of Frames: 151\n",
      "Processing file data/UCF101_subset/val/BalanceBeam/v_BalanceBeam_g16_c01.avi number of Frames: 124\n",
      "Processing file data/UCF101_subset/val/BalanceBeam/v_BalanceBeam_g22_c02.avi number of Frames: 96\n",
      "Processing file data/UCF101_subset/val/BalanceBeam/v_BalanceBeam_g13_c02.avi number of Frames: 125\n",
      "Processing file data/UCF101_subset/test/BabyCrawling/v_BabyCrawling_g04_c01.avi number of Frames: 127\n",
      "Processing file data/UCF101_subset/test/BabyCrawling/v_BabyCrawling_g19_c02.avi number of Frames: 170\n",
      "Processing file data/UCF101_subset/test/BabyCrawling/v_BabyCrawling_g19_c04.avi number of Frames: 144\n",
      "Processing file data/UCF101_subset/test/BabyCrawling/v_BabyCrawling_g22_c04.avi number of Frames: 193\n",
      "Processing file data/UCF101_subset/test/BabyCrawling/v_BabyCrawling_g22_c02.avi number of Frames: 197\n",
      "Processing file data/UCF101_subset/test/BabyCrawling/v_BabyCrawling_g22_c06.avi number of Frames: 92\n",
      "Processing file data/UCF101_subset/test/BabyCrawling/v_BabyCrawling_g03_c01.avi number of Frames: 215\n",
      "Processing file data/UCF101_subset/test/BabyCrawling/v_BabyCrawling_g03_c03.avi number of Frames: 198\n",
      "Processing file data/UCF101_subset/test/BabyCrawling/v_BabyCrawling_g04_c03.avi number of Frames: 159\n",
      "Processing file data/UCF101_subset/test/ApplyEyeMakeup/v_ApplyEyeMakeup_g23_c02.avi number of Frames: 131\n",
      "Processing file data/UCF101_subset/test/ApplyEyeMakeup/v_ApplyEyeMakeup_g03_c03.avi number of Frames: 115\n",
      "Processing file data/UCF101_subset/test/ApplyEyeMakeup/v_ApplyEyeMakeup_g23_c04.avi number of Frames: 200\n",
      "Processing file data/UCF101_subset/test/ApplyEyeMakeup/v_ApplyEyeMakeup_g23_c06.avi number of Frames: 140\n",
      "Processing file data/UCF101_subset/test/ApplyEyeMakeup/v_ApplyEyeMakeup_g03_c01.avi number of Frames: 209\n",
      "Processing file data/UCF101_subset/test/ApplyEyeMakeup/v_ApplyEyeMakeup_g03_c05.avi number of Frames: 146\n",
      "Processing file data/UCF101_subset/test/Archery/v_Archery_g21_c04.avi number of Frames: 205\n",
      "Processing file data/UCF101_subset/test/Archery/v_Archery_g16_c01.avi number of Frames: 151\n",
      "Processing file data/UCF101_subset/test/Archery/v_Archery_g17_c02.avi number of Frames: 94\n",
      "Processing file data/UCF101_subset/test/Archery/v_Archery_g16_c03.avi number of Frames: 206\n",
      "Processing file data/UCF101_subset/test/Archery/v_Archery_g16_c05.avi number of Frames: 262\n",
      "Processing file data/UCF101_subset/test/Archery/v_Archery_g17_c04.avi number of Frames: 66\n",
      "Processing file data/UCF101_subset/test/Archery/v_Archery_g21_c02.avi number of Frames: 175\n",
      "Processing file data/UCF101_subset/test/ApplyLipstick/v_ApplyLipstick_g14_c01.avi number of Frames: 176\n",
      "Processing file data/UCF101_subset/test/ApplyLipstick/v_ApplyLipstick_g16_c04.avi number of Frames: 173\n",
      "Processing file data/UCF101_subset/test/ApplyLipstick/v_ApplyLipstick_g14_c03.avi number of Frames: 174\n",
      "Processing file data/UCF101_subset/test/ApplyLipstick/v_ApplyLipstick_g16_c02.avi number of Frames: 165\n",
      "Processing file data/UCF101_subset/test/BalanceBeam/v_BalanceBeam_g11_c04.avi number of Frames: 116\n",
      "Processing file data/UCF101_subset/test/BalanceBeam/v_BalanceBeam_g20_c01.avi number of Frames: 84\n",
      "Processing file data/UCF101_subset/test/BalanceBeam/v_BalanceBeam_g20_c03.avi number of Frames: 100\n",
      "Processing file data/UCF101_subset/test/BalanceBeam/v_BalanceBeam_g11_c02.avi number of Frames: 68\n",
      "Min number frames 51\n"
     ]
    }
   ],
   "source": [
    "path_files = \"data/UCF101_subset\"\n",
    "video_dict, class_labels = frames_convert_and_create_dataset_dictionary(path_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(video_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['video', 'labels'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_dict[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 224, 224, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_dict[0]['video'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BabyCrawling'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_dict[0]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 224, 224, 3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_frames, height, width, channels =  video_dict[0]['video'].shape\n",
    "num_frames, height, width, channels "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Video Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes: ['ApplyEyeMakeup', 'ApplyLipstick', 'Archery', 'BabyCrawling', 'BalanceBeam'].\n"
     ]
    }
   ],
   "source": [
    "class_labels = sorted(class_labels)\n",
    "label2id = {label: i for i, label in enumerate(class_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "print(f\"Unique classes: {list(label2id.keys())}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting to class labels: 100%|██████████| 195/195 [00:00<00:00, 613.94 examples/s]\n",
      "Map: 100%|██████████| 195/195 [03:27<00:00,  1.06s/ examples]\n",
      "Map: 100%|██████████| 195/195 [01:31<00:00,  2.12 examples/s]\n"
     ]
    }
   ],
   "source": [
    "shuffled_dataset = create_dataset(video_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': ClassLabel(names=['ApplyEyeMakeup', 'ApplyLipstick', 'Archery', 'BabyCrawling', 'BalanceBeam'], id=None),\n",
       " 'pixel_values': Sequence(feature=Sequence(feature=Sequence(feature=Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), length=-1, id=None), length=-1, id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_dataset['train'].features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VivitForVideoClassification were not initialized from the model checkpoint at google/vivit-b-16x2-kinetics400 and are newly initialized because the shapes did not match:\n",
      "- vivit.embeddings.position_embeddings: found shape torch.Size([1, 3137, 768]) in the checkpoint and torch.Size([1, 981, 768]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = model_configuration.initialise_model(shuffled_dataset, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install accelerate==1.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_output_dir = \"/tmp/results\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=training_output_dir,         \n",
    "    num_train_epochs=3,             \n",
    "    per_device_train_batch_size=2,   \n",
    "    per_device_eval_batch_size=2,    \n",
    "    learning_rate=5e-05,            \n",
    "    weight_decay=0.01,              \n",
    "    logging_dir=\"./logs\",           \n",
    "    logging_steps=10,                \n",
    "    seed=42,                       \n",
    "    eval_strategy=\"steps\",    \n",
    "    eval_steps=10,                   \n",
    "    warmup_steps=int(0.1 * 20),      \n",
    "    optim=\"adamw_torch\",          \n",
    "    lr_scheduler_type=\"linear\",      \n",
    "    fp16=True,  \n",
    "    report_to=\"wandb\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-05, betas=(0.9, 0.999), eps=1e-08)\n",
    "# Define the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                      \n",
    "    args=training_args,              \n",
    "    train_dataset=shuffled_dataset[\"train\"],      \n",
    "    eval_dataset=shuffled_dataset[\"test\"],       \n",
    "    optimizers=(optimizer, None),  \n",
    "    compute_metrics = compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:4v4hltmb) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lucky-cherry-1</strong> at: <a href='https://wandb.ai/anookinskywalker-jaypee-institute-of-information-technology/ViViT/runs/4v4hltmb' target=\"_blank\">https://wandb.ai/anookinskywalker-jaypee-institute-of-information-technology/ViViT/runs/4v4hltmb</a><br/> View project at: <a href='https://wandb.ai/anookinskywalker-jaypee-institute-of-information-technology/ViViT' target=\"_blank\">https://wandb.ai/anookinskywalker-jaypee-institute-of-information-technology/ViViT</a><br/>Synced 4 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241109_144946-4v4hltmb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:4v4hltmb). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/darth/Major01/wandb/run-20241109_145040-pm22xv17</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/anookinskywalker-jaypee-institute-of-information-technology/ViViT/runs/pm22xv17' target=\"_blank\">morning-valley-2</a></strong> to <a href='https://wandb.ai/anookinskywalker-jaypee-institute-of-information-technology/ViViT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/anookinskywalker-jaypee-institute-of-information-technology/ViViT' target=\"_blank\">https://wandb.ai/anookinskywalker-jaypee-institute-of-information-technology/ViViT</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/anookinskywalker-jaypee-institute-of-information-technology/ViViT/runs/pm22xv17' target=\"_blank\">https://wandb.ai/anookinskywalker-jaypee-institute-of-information-technology/ViViT/runs/pm22xv17</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/anookinskywalker-jaypee-institute-of-information-technology/ViViT/runs/pm22xv17?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x78746036bac0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb_key =  os.getenv(\"WANDB_API_KEY\")\n",
    "wandb.login(key=wandb_key)\n",
    "\n",
    "PROJECT = \"ViViT\"\n",
    "MODEL_NAME = \"google/vivit-b-16x2-kinetics400\"\n",
    "DATASET = \"sayakpaul/ucf101-subset\"\n",
    "\n",
    "wandb.init(project=PROJECT, # the project I am working on\n",
    "           tags=[MODEL_NAME, DATASET],\n",
    "           notes =\"Fine tuning ViViT with ucf101-subset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-05, betas=(0.9, 0.999), eps=1e-08)\n",
    "# Define the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                      \n",
    "    args=training_args,              \n",
    "    train_dataset=shuffled_dataset[\"train\"],      \n",
    "    eval_dataset=shuffled_dataset[\"test\"],       \n",
    "    optimizers=(optimizer, None),  \n",
    "    compute_metrics = compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:pm22xv17) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">morning-valley-2</strong> at: <a href='https://wandb.ai/anookinskywalker-jaypee-institute-of-information-technology/ViViT/runs/pm22xv17' target=\"_blank\">https://wandb.ai/anookinskywalker-jaypee-institute-of-information-technology/ViViT/runs/pm22xv17</a><br/> View project at: <a href='https://wandb.ai/anookinskywalker-jaypee-institute-of-information-technology/ViViT' target=\"_blank\">https://wandb.ai/anookinskywalker-jaypee-institute-of-information-technology/ViViT</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241109_145040-pm22xv17/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:pm22xv17). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/darth/Major01/wandb/run-20241109_145121-cvd53vf6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/anookinskywalker-jaypee-institute-of-information-technology/ViViT/runs/cvd53vf6' target=\"_blank\">cerulean-spaceship-3</a></strong> to <a href='https://wandb.ai/anookinskywalker-jaypee-institute-of-information-technology/ViViT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/anookinskywalker-jaypee-institute-of-information-technology/ViViT' target=\"_blank\">https://wandb.ai/anookinskywalker-jaypee-institute-of-information-technology/ViViT</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/anookinskywalker-jaypee-institute-of-information-technology/ViViT/runs/cvd53vf6' target=\"_blank\">https://wandb.ai/anookinskywalker-jaypee-institute-of-information-technology/ViViT/runs/cvd53vf6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "  4%|▍         | 10/264 [00:11<04:20,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.648, 'grad_norm': 33.411678314208984, 'learning_rate': 4.885496183206107e-05, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      "  4%|▍         | 10/264 [00:20<04:20,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.286291480064392, 'eval_accuracy': 0.45, 'eval_runtime': 9.2477, 'eval_samples_per_second': 2.163, 'eval_steps_per_second': 1.081, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 20/264 [00:30<04:39,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1584, 'grad_norm': 44.176692962646484, 'learning_rate': 4.713740458015267e-05, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      "  8%|▊         | 20/264 [00:39<04:39,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1101562976837158, 'eval_accuracy': 0.65, 'eval_runtime': 9.1036, 'eval_samples_per_second': 2.197, 'eval_steps_per_second': 1.098, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 30/264 [00:50<04:23,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0708, 'grad_norm': 17.005741119384766, 'learning_rate': 4.522900763358779e-05, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 11%|█▏        | 30/264 [00:59<04:23,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.744799792766571, 'eval_accuracy': 0.8, 'eval_runtime': 9.2289, 'eval_samples_per_second': 2.167, 'eval_steps_per_second': 1.084, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 40/264 [01:09<04:21,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6432, 'grad_norm': 26.458826065063477, 'learning_rate': 4.332061068702291e-05, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 15%|█▌        | 40/264 [01:18<04:21,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6257659792900085, 'eval_accuracy': 0.8, 'eval_runtime': 9.169, 'eval_samples_per_second': 2.181, 'eval_steps_per_second': 1.091, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 50/264 [01:29<04:03,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5213, 'grad_norm': 26.027969360351562, 'learning_rate': 4.1412213740458014e-05, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 19%|█▉        | 50/264 [01:38<04:03,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.553997814655304, 'eval_accuracy': 0.75, 'eval_runtime': 9.3817, 'eval_samples_per_second': 2.132, 'eval_steps_per_second': 1.066, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 60/264 [01:48<03:51,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4425, 'grad_norm': 9.949991226196289, 'learning_rate': 3.950381679389313e-05, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 23%|██▎       | 60/264 [01:58<03:51,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4210754334926605, 'eval_accuracy': 0.9, 'eval_runtime': 9.385, 'eval_samples_per_second': 2.131, 'eval_steps_per_second': 1.066, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 70/264 [02:08<03:46,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4241, 'grad_norm': 23.707199096679688, 'learning_rate': 3.7595419847328244e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 27%|██▋       | 70/264 [02:18<03:46,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.41632309556007385, 'eval_accuracy': 0.85, 'eval_runtime': 9.2678, 'eval_samples_per_second': 2.158, 'eval_steps_per_second': 1.079, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 80/264 [02:28<03:30,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5195, 'grad_norm': 31.68621253967285, 'learning_rate': 3.568702290076336e-05, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 30%|███       | 80/264 [02:37<03:30,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6138721704483032, 'eval_accuracy': 0.7, 'eval_runtime': 9.3564, 'eval_samples_per_second': 2.138, 'eval_steps_per_second': 1.069, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 90/264 [02:47<03:24,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5145, 'grad_norm': 0.7145484089851379, 'learning_rate': 3.3778625954198475e-05, 'epoch': 1.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 34%|███▍      | 90/264 [02:56<03:24,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.26420384645462036, 'eval_accuracy': 0.95, 'eval_runtime': 9.1963, 'eval_samples_per_second': 2.175, 'eval_steps_per_second': 1.087, 'epoch': 1.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 100/264 [03:07<03:06,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1985, 'grad_norm': 2.672175168991089, 'learning_rate': 3.187022900763359e-05, 'epoch': 1.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 38%|███▊      | 100/264 [03:16<03:06,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.20445671677589417, 'eval_accuracy': 0.95, 'eval_runtime': 9.3228, 'eval_samples_per_second': 2.145, 'eval_steps_per_second': 1.073, 'epoch': 1.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 110/264 [03:26<02:55,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0702, 'grad_norm': 13.60689926147461, 'learning_rate': 2.9961832061068706e-05, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 42%|████▏     | 110/264 [03:36<02:55,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.37611904740333557, 'eval_accuracy': 0.75, 'eval_runtime': 9.3211, 'eval_samples_per_second': 2.146, 'eval_steps_per_second': 1.073, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 120/264 [03:46<02:46,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.155, 'grad_norm': 0.09088923037052155, 'learning_rate': 2.805343511450382e-05, 'epoch': 1.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 45%|████▌     | 120/264 [03:55<02:46,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.15165233612060547, 'eval_accuracy': 0.95, 'eval_runtime': 9.2015, 'eval_samples_per_second': 2.174, 'eval_steps_per_second': 1.087, 'epoch': 1.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 130/264 [04:06<02:33,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.048, 'grad_norm': 0.4784669876098633, 'learning_rate': 2.6145038167938934e-05, 'epoch': 1.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 49%|████▉     | 130/264 [04:15<02:33,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.21213588118553162, 'eval_accuracy': 0.9, 'eval_runtime': 9.3626, 'eval_samples_per_second': 2.136, 'eval_steps_per_second': 1.068, 'epoch': 1.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 140/264 [04:25<02:24,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1789, 'grad_norm': 0.45993635058403015, 'learning_rate': 2.4236641221374046e-05, 'epoch': 1.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 53%|█████▎    | 140/264 [04:34<02:24,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1006142646074295, 'eval_accuracy': 1.0, 'eval_runtime': 9.2133, 'eval_samples_per_second': 2.171, 'eval_steps_per_second': 1.085, 'epoch': 1.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 150/264 [04:45<02:09,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0403, 'grad_norm': 0.5529696345329285, 'learning_rate': 2.232824427480916e-05, 'epoch': 1.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 57%|█████▋    | 150/264 [04:54<02:09,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.05935673788189888, 'eval_accuracy': 1.0, 'eval_runtime': 9.2964, 'eval_samples_per_second': 2.151, 'eval_steps_per_second': 1.076, 'epoch': 1.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 160/264 [05:04<01:58,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0154, 'grad_norm': 0.15790769457817078, 'learning_rate': 2.0419847328244277e-05, 'epoch': 1.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 61%|██████    | 160/264 [05:14<01:58,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.038683317601680756, 'eval_accuracy': 1.0, 'eval_runtime': 9.2913, 'eval_samples_per_second': 2.153, 'eval_steps_per_second': 1.076, 'epoch': 1.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 170/264 [05:24<01:47,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0172, 'grad_norm': 1.2312973737716675, 'learning_rate': 1.851145038167939e-05, 'epoch': 1.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 64%|██████▍   | 170/264 [05:33<01:47,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.036098577082157135, 'eval_accuracy': 1.0, 'eval_runtime': 9.2028, 'eval_samples_per_second': 2.173, 'eval_steps_per_second': 1.087, 'epoch': 1.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 180/264 [05:43<01:35,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0076, 'grad_norm': 0.11387345939874649, 'learning_rate': 1.6603053435114505e-05, 'epoch': 2.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 68%|██████▊   | 180/264 [05:52<01:35,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06169138103723526, 'eval_accuracy': 1.0, 'eval_runtime': 9.2923, 'eval_samples_per_second': 2.152, 'eval_steps_per_second': 1.076, 'epoch': 2.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 190/264 [06:03<01:26,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0061, 'grad_norm': 0.06312558799982071, 'learning_rate': 1.4694656488549618e-05, 'epoch': 2.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 72%|███████▏  | 190/264 [06:12<01:26,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.047100018709897995, 'eval_accuracy': 1.0, 'eval_runtime': 9.2475, 'eval_samples_per_second': 2.163, 'eval_steps_per_second': 1.081, 'epoch': 2.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 200/264 [06:22<01:13,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0034, 'grad_norm': 0.23209892213344574, 'learning_rate': 1.2786259541984732e-05, 'epoch': 2.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 76%|███████▌  | 200/264 [06:31<01:13,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.05480971187353134, 'eval_accuracy': 0.95, 'eval_runtime': 9.2679, 'eval_samples_per_second': 2.158, 'eval_steps_per_second': 1.079, 'epoch': 2.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 210/264 [06:42<01:01,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0048, 'grad_norm': 0.023650895804166794, 'learning_rate': 1.0877862595419848e-05, 'epoch': 2.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 80%|███████▉  | 210/264 [06:51<01:01,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06784896552562714, 'eval_accuracy': 1.0, 'eval_runtime': 9.2453, 'eval_samples_per_second': 2.163, 'eval_steps_per_second': 1.082, 'epoch': 2.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 220/264 [07:01<00:50,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0054, 'grad_norm': 0.07987233251333237, 'learning_rate': 8.969465648854961e-06, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 83%|████████▎ | 220/264 [07:10<00:50,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.047449398785829544, 'eval_accuracy': 1.0, 'eval_runtime': 9.2684, 'eval_samples_per_second': 2.158, 'eval_steps_per_second': 1.079, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 230/264 [07:21<00:38,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.002, 'grad_norm': 0.06713747978210449, 'learning_rate': 7.061068702290078e-06, 'epoch': 2.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 87%|████████▋ | 230/264 [07:30<00:38,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.02948460541665554, 'eval_accuracy': 1.0, 'eval_runtime': 9.2905, 'eval_samples_per_second': 2.153, 'eval_steps_per_second': 1.076, 'epoch': 2.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 240/264 [07:40<00:28,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0025, 'grad_norm': 0.05533894896507263, 'learning_rate': 5.1526717557251914e-06, 'epoch': 2.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 91%|█████████ | 240/264 [07:49<00:28,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.023304175585508347, 'eval_accuracy': 1.0, 'eval_runtime': 9.1685, 'eval_samples_per_second': 2.181, 'eval_steps_per_second': 1.091, 'epoch': 2.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 250/264 [08:00<00:15,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0024, 'grad_norm': 0.07648076117038727, 'learning_rate': 3.2442748091603052e-06, 'epoch': 2.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 95%|█████████▍| 250/264 [08:09<00:15,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.020380020141601562, 'eval_accuracy': 1.0, 'eval_runtime': 9.3042, 'eval_samples_per_second': 2.15, 'eval_steps_per_second': 1.075, 'epoch': 2.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 260/264 [08:19<00:04,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0015, 'grad_norm': 0.13047854602336884, 'learning_rate': 1.3358778625954198e-06, 'epoch': 2.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 98%|█████████▊| 260/264 [08:29<00:04,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.018836164847016335, 'eval_accuracy': 1.0, 'eval_runtime': 9.2971, 'eval_samples_per_second': 2.151, 'eval_steps_per_second': 1.076, 'epoch': 2.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 264/264 [08:32<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 512.519, 'train_samples_per_second': 1.024, 'train_steps_per_second': 0.515, 'train_loss': 0.29176113641623297, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▄▅▅▅▇▆▄▇▇▅▇▇██████▇██████</td></tr><tr><td>eval/loss</td><td>█▇▅▄▄▃▃▄▂▂▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▅▁▄▃██▅▇▃▆▆▃▇▄▆▆▃▆▅▅▅▅▆▃▆▆</td></tr><tr><td>eval/samples_per_second</td><td>▄█▅▆▁▁▄▂▆▂▃▆▂▅▃▃▅▃▄▄▄▄▃▆▃▃</td></tr><tr><td>eval/steps_per_second</td><td>▄█▅▆▁▁▄▂▆▃▃▆▁▅▃▃▆▃▄▄▅▄▃▆▃▃</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/grad_norm</td><td>▆█▄▅▅▃▅▆▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▇▆▆▆▅▅▅▅▄▄▄▄▃▃▃▂▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▆▆▄▃▃▃▃▃▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>1</td></tr><tr><td>eval/loss</td><td>0.01884</td></tr><tr><td>eval/runtime</td><td>9.2971</td></tr><tr><td>eval/samples_per_second</td><td>2.151</td></tr><tr><td>eval/steps_per_second</td><td>1.076</td></tr><tr><td>total_flos</td><td>4.12495606301184e+17</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/global_step</td><td>264</td></tr><tr><td>train/grad_norm</td><td>0.13048</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0015</td></tr><tr><td>train_loss</td><td>0.29176</td></tr><tr><td>train_runtime</td><td>512.519</td></tr><tr><td>train_samples_per_second</td><td>1.024</td></tr><tr><td>train_steps_per_second</td><td>0.515</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cerulean-spaceship-3</strong> at: <a href='https://wandb.ai/anookinskywalker-jaypee-institute-of-information-technology/ViViT/runs/cvd53vf6' target=\"_blank\">https://wandb.ai/anookinskywalker-jaypee-institute-of-information-technology/ViViT/runs/cvd53vf6</a><br/> View project at: <a href='https://wandb.ai/anookinskywalker-jaypee-institute-of-information-technology/ViViT' target=\"_blank\">https://wandb.ai/anookinskywalker-jaypee-institute-of-information-technology/ViViT</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241109_145121-cvd53vf6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with wandb.init(project=PROJECT, job_type=\"train\", # the project I am working on\n",
    "           tags=[MODEL_NAME, DATASET],\n",
    "           notes =f\"Fine tuning {MODEL_NAME} with {DATASET}.\"):\n",
    "           train_results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =         3.0\n",
      "  total_flos               = 384166470GF\n",
      "  train_loss               =      0.2918\n",
      "  train_runtime            =  0:08:32.51\n",
      "  train_samples_per_second =       1.024\n",
      "  train_steps_per_second   =       0.515\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"model\")\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_path = \"./model\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/darth/Major01/wandb/run-20241109_150119-zq3is4os</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/anookinskywalker-jaypee-institute-of-information-technology/ViViT/runs/zq3is4os' target=\"_blank\">clear-energy-4</a></strong> to <a href='https://wandb.ai/anookinskywalker-jaypee-institute-of-information-technology/ViViT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/anookinskywalker-jaypee-institute-of-information-technology/ViViT' target=\"_blank\">https://wandb.ai/anookinskywalker-jaypee-institute-of-information-technology/ViViT</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/anookinskywalker-jaypee-institute-of-information-technology/ViViT/runs/zq3is4os' target=\"_blank\">https://wandb.ai/anookinskywalker-jaypee-institute-of-information-technology/ViViT/runs/zq3is4os</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./model)... Done. 0.5s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">clear-energy-4</strong> at: <a href='https://wandb.ai/anookinskywalker-jaypee-institute-of-information-technology/ViViT/runs/zq3is4os' target=\"_blank\">https://wandb.ai/anookinskywalker-jaypee-institute-of-information-technology/ViViT/runs/zq3is4os</a><br/> View project at: <a href='https://wandb.ai/anookinskywalker-jaypee-institute-of-information-technology/ViViT' target=\"_blank\">https://wandb.ai/anookinskywalker-jaypee-institute-of-information-technology/ViViT</a><br/>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241109_150119-zq3is4os/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with wandb.init(project=PROJECT, job_type=\"models\"):\n",
    "  artifact = wandb.Artifact(\"ViViT-Fine-tuned\", type=\"model\")\n",
    "  artifact.add_dir(custom_path)\n",
    "  wandb.save(custom_path)\n",
    "  wandb.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "major",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
